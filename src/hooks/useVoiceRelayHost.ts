import { useCallback, useEffect, useRef, useState } from "react";
import { useSocket } from "./useSocket";
import { useWebRTC } from "./useWebRTC";
import { useOpenAIRealtime } from "./useOpenAIRealtime";
import { AudioProcessor } from "@/utils/audioProcessor";
import { BUFFER_SIZE, MIN_BYTES } from "@/app/constants/audio";

interface VoiceRelayState {
  isHostReady: boolean;
  isGuestConnected: boolean;
  isAIConnected: boolean;
  isRelayActive: boolean;
  currentSpeaker: "guest" | "ai" | "none";
  error: string | null;

  guestAudioLevel: number; // 0-100 ë²”ìœ„ì˜ ê²ŒìŠ¤íŠ¸ ìŒì„± ë ˆë²¨ (í˜¸ìŠ¤íŠ¸ì—ì„œ ìˆ˜ì‹ )
}

export interface VoiceRelayHostHook extends VoiceRelayState {
  webRTCState: {
    localStream: MediaStream | null;
    remoteStream: MediaStream | null;
    connectionState: RTCPeerConnectionState;
  };
  openAIState: {
    isConnected: boolean;
    conversationId: string | null;
  };
  initialize: (hostName: string) => Promise<string | null>;
  startWebRTCConnection: () => Promise<void>;
  startVoiceRelay: () => void;

  setupAIAudioOutput: () => MediaStream | null;
  clearError: () => void;
  cleanup: () => void;
}

export function useVoiceRelayHost(): VoiceRelayHostHook {
  const audioProcessorRef = useRef<AudioProcessor | null>(null);
  const previousRelayActiveRef = useRef(false);
  const conversationTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const isProcessingResponseRef = useRef(false);
  const audioBufferCountRef = useRef(0);

  const [relayState, setRelayState] = useState<VoiceRelayState>({
    isHostReady: false,
    isGuestConnected: false,
    isAIConnected: false,
    isRelayActive: false,
    currentSpeaker: "none",
    error: null,

    guestAudioLevel: 0,
  });

  // ê° ëª¨ë“ˆ í›… ì‚¬ìš©
  const { socket, connectionState, createRoom } = useSocket();

  // AI ì‘ë‹µ ì˜¤ë””ì˜¤ ì½œë°± ì„¤ì •
  const handleAIAudioResponse = (audioData: string) => {
    const processor = audioProcessorRef.current;
    if (processor && connectionState.role === "host") {
      console.log("ğŸ¤ !!! AI ì‘ë‹µ ì˜¤ë””ì˜¤ ìˆ˜ì‹ , ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì¬ìƒ");
      processor.enqueueAIResponse(audioData);
      setRelayState((prev) => ({ ...prev, currentSpeaker: "ai" }));

      // ì‘ë‹µ ì‹œì‘ì‹œ í”Œë˜ê·¸ ì„¤ì •
      if (!isProcessingResponseRef.current) {
        isProcessingResponseRef.current = true;
        console.log("ğŸ¤– !!! AI ì‘ë‹µ ì²˜ë¦¬ ì‹œì‘");
      }
    } else {
      console.log("ğŸ¤ !!! AI ì‘ë‹µ ì˜¤ë””ì˜¤ ìˆ˜ì‹ , ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì¬ìƒ ë¶ˆê°€", {
        processor: !!processor,
        connectionState,
      });
    }
  };

  // AI ì‘ë‹µ ì™„ë£Œ ì½œë°± ì„¤ì •
  const handleAIResponseComplete = useCallback(() => {
    console.log("âœ… AI ì‘ë‹µ ì™„ë£Œ, ë‹¤ìŒ ìš”ì²­ ì¤€ë¹„");
    isProcessingResponseRef.current = false;
    setRelayState((prev) => ({ ...prev, currentSpeaker: "none" }));
  }, []);

  const webRTC = useWebRTC(socket, connectionState.role, connectionState.remoteSocketId);
  const openAI = useOpenAIRealtime({
    onAudioResponse: handleAIAudioResponse,
    onResponseComplete: handleAIResponseComplete,
  });

  // ê²ŒìŠ¤íŠ¸ìš© ë§ˆì´í¬ ë ˆë²¨ ëª¨ë‹ˆí„°ë§ ì‹œì‘
  const startMicrophoneMonitoring = useCallback(() => {
    if (connectionState.role !== "guest" || !webRTC.localStream) {
      return;
    }

    const processor = audioProcessorRef.current;
    if (!processor) {
      console.error("ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤");
      return;
    }

    try {
      console.log("ğŸ¤ !!! ê²ŒìŠ¤íŠ¸ ë§ˆì´í¬ ë ˆë²¨ ëª¨ë‹ˆí„°ë§ ì‹œì‘");

      processor.setupInputProcessor(webRTC.localStream, (audioBuffer) => {
        // ì˜¤ë””ì˜¤ ë°ì´í„° ë¶„ì„
        const dataView = new DataView(audioBuffer);
        const bufferSize = audioBuffer.byteLength;
        let maxAmplitude = 0;

        // PCM16 ë°ì´í„°ì—ì„œ ì§„í­ ì²´í¬ (2ë°”ì´íŠ¸ì”©)
        for (let i = 0; i < bufferSize; i += 2) {
          const sample = Math.abs(dataView.getInt16(i, true)); // little-endian
          maxAmplitude = Math.max(maxAmplitude, sample);
        }

        // ì§„í­ì„ 0-100 ë ˆë²¨ë¡œ ë³€í™˜ (32767ì´ ìµœëŒ€ê°’)
        const level = Math.min(100, (maxAmplitude / 32767) * 100);
        console.log("ğŸ¤ !!! ê²ŒìŠ¤íŠ¸ ë§ˆì´í¬ ë ˆë²¨:", level, relayState.currentSpeaker);
        // í˜¸ìŠ¤íŠ¸ëŠ” ê²ŒìŠ¤íŠ¸ ë§ˆì´í¬ ë ˆë²¨ì„ ë”°ë¡œ ì €ì¥í•˜ì§€ ì•ŠìŒ (guestAudioLevelë§Œ ì‚¬ìš©)
      });

      console.log("âœ… ê²ŒìŠ¤íŠ¸ ë§ˆì´í¬ ëª¨ë‹ˆí„°ë§ ì„¤ì • ì™„ë£Œ");
    } catch (error) {
      console.error("âŒ ê²ŒìŠ¤íŠ¸ ë§ˆì´í¬ ëª¨ë‹ˆí„°ë§ ì„¤ì • ì‹¤íŒ¨:", error);
    }
  }, [connectionState.role, webRTC.localStream]);

  // OpenAI ì‘ë‹µì„ ê²ŒìŠ¤íŠ¸ë¡œ ì „ì†¡í•˜ê¸° ìœ„í•œ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ìƒì„±
  const setupAIAudioOutput = useCallback(() => {
    if (connectionState.role !== "host") {
      console.warn("í˜¸ìŠ¤íŠ¸ë§Œ AI ì‘ë‹µ ì¶œë ¥ì„ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤");
      return null;
    }

    const processor = audioProcessorRef.current;
    if (!processor || !webRTC.isConnected) {
      console.error("ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œê°€ ì—†ê±°ë‚˜ WebRTCê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤", {
        processor: !!processor,
        webRTCConnected: webRTC.isConnected,
      });
      return null;
    }

    try {
      // AI ì‘ë‹µìš© ì‹¤ì‹œê°„ MediaStream ìƒì„±
      const aiResponseStream = processor.createAIResponseStream();

      if (!aiResponseStream) {
        console.error("AI ì‘ë‹µ ìŠ¤íŠ¸ë¦¼ ìƒì„± ì‹¤íŒ¨");
        return null;
      }

      // WebRTCë¥¼ í†µí•´ ì†¡ì‹  ìŠ¤íŠ¸ë¦¼ ì„¤ì •
      webRTC.setOutgoingStream(aiResponseStream);

      console.log("!! âœ… AI ì‘ë‹µ ì¶œë ¥ ì„¤ì • ì™„ë£Œ");
      return aiResponseStream;
    } catch (error) {
      console.error("âŒ AI ì‘ë‹µ ì¶œë ¥ ì„¤ì • ì‹¤íŒ¨:", error);
      return null;
    }
  }, [connectionState.role, webRTC.isConnected]);

  // ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
  useEffect(() => {
    const processor = new AudioProcessor();
    audioProcessorRef.current = processor;

    return () => {
      processor.cleanup();
    };
  }, []);

  // ìƒíƒœ ì—…ë°ì´íŠ¸
  useEffect(() => {
    // ê²ŒìŠ¤íŠ¸ ì—°ê²° ìƒíƒœ í™•ì¸ ë¡œì§ ê°œì„ 
    const isGuestConnected =
      connectionState.role === "host"
        ? !!connectionState.remoteSocketId // í˜¸ìŠ¤íŠ¸: ê²ŒìŠ¤íŠ¸ ì†Œì¼“ IDê°€ ìˆìœ¼ë©´ ì—°ê²°ë¨
        : connectionState.role === "guest" && webRTC.isConnected; // ê²ŒìŠ¤íŠ¸: ìì‹ ì˜ ì—°ê²° ìƒíƒœ

    const isRelayActive = webRTC.isConnected && openAI.isConnected && openAI.isSessionActive;

    // í˜¸ìŠ¤íŠ¸ì—ì„œ ë¦´ë ˆì´ê°€ í™œì„±í™”ë˜ë©´ AI ì‘ë‹µ ì¶œë ¥ ìë™ ì„¤ì •
    if (isRelayActive && connectionState.role === "host" && !previousRelayActiveRef.current) {
      console.log("ğŸ”„ ë¦´ë ˆì´ í™œì„±í™”ë¨, AI ì‘ë‹µ ì¶œë ¥ ì„¤ì • ì¤‘...");
      setupAIAudioOutput();
    }

    if (
      connectionState.role === "guest" &&
      webRTC.isConnected &&
      webRTC.localStream &&
      !previousRelayActiveRef.current
    ) {
      console.log("ğŸ”„ !!! ê²ŒìŠ¤íŠ¸ WebRTC ì—°ê²°ë¨, ë§ˆì´í¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘ ì¤‘...");
      startMicrophoneMonitoring();
    }

    // ë¦´ë ˆì´ê°€ ë¹„í™œì„±í™”ë˜ë©´ ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ ì •ë¦¬
    if (!isRelayActive && previousRelayActiveRef.current && audioProcessorRef.current) {
      console.log("ğŸ”„ ë¦´ë ˆì´ ë¹„í™œì„±í™”ë¨, ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ ì •ë¦¬ ì¤‘...");

      // íƒ€ì´ë¨¸ ì •ë¦¬
      if (conversationTimeoutRef.current) {
        clearTimeout(conversationTimeoutRef.current);
        conversationTimeoutRef.current = null;
      }

      // ìƒíƒœ ë¦¬ì…‹
      isProcessingResponseRef.current = false;
      audioBufferCountRef.current = 0;

      // ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ ì¬ì´ˆê¸°í™”
      audioProcessorRef.current.cleanup();
      const newProcessor = new AudioProcessor();
      audioProcessorRef.current = newProcessor;
      newProcessor.initializeAudioContext();
    }

    // ì´ì „ ìƒíƒœ ì¶”ì  ì—…ë°ì´íŠ¸
    previousRelayActiveRef.current = isRelayActive;

    setRelayState((prev) => ({
      ...prev,
      isHostReady: connectionState.role === "host" && connectionState.isConnected,
      isGuestConnected,
      isAIConnected: openAI.isConnected && openAI.isSessionActive,
      isRelayActive,
      error: webRTC.connectionState === "failed" ? "WebRTC ì—°ê²° ì‹¤íŒ¨" : openAI.lastError,
    }));
  }, [
    connectionState,
    webRTC.isConnected,
    webRTC.connectionState,
    webRTC.localStream,
    webRTC.remoteStream,
    openAI.isConnected,
    openAI.isSessionActive,
    openAI.lastError,
    setupAIAudioOutput,
    startMicrophoneMonitoring,
  ]);

  // í˜¸ìŠ¤íŠ¸: ë°© ìƒì„± ë° ì‹œìŠ¤í…œ ì´ˆê¸°í™”
  const initialize = useCallback(
    async (hostName: string): Promise<string | null> => {
      try {
        // ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        const processor = audioProcessorRef.current;
        if (!processor) {
          throw new Error("ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤");
        }

        const audioInitialized = await processor.initializeAudioContext();
        if (!audioInitialized) {
          throw new Error("ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨");
        }

        // OpenAI ì—°ê²°
        const aiConnected = await openAI.connectToOpenAI();
        if (!aiConnected) {
          throw new Error("OpenAI ì—°ê²° ì‹¤íŒ¨");
        }

        // ì†Œì¼“ ë°© ìƒì„±
        const roomId = await new Promise<string>((resolve, reject) => {
          createRoom(hostName, (roomId) => {
            console.log("í˜¸ìŠ¤íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ, ë°© ID:", roomId);
            resolve(roomId);
          });

          // íƒ€ì„ì•„ì›ƒ ì„¤ì •
          setTimeout(() => {
            reject(new Error("ë°© ìƒì„± íƒ€ì„ì•„ì›ƒ"));
          }, 5000);
        });

        return roomId;
      } catch (error) {
        console.error("í˜¸ìŠ¤íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨:", error);
        setRelayState((prev) => ({
          ...prev,
          error: error instanceof Error ? error.message : "í˜¸ìŠ¤íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨",
        }));
        return null;
      }
    },
    [createRoom, openAI],
  );

  // WebRTC ì—°ê²° ì‹œì‘ (í˜¸ìŠ¤íŠ¸ê°€ ê²ŒìŠ¤íŠ¸ ì°¸ì—¬ í›„ í˜¸ì¶œ)
  const startWebRTCConnection = useCallback(async () => {
    if (connectionState.role !== "host" || !connectionState.remoteSocketId) {
      console.warn("í˜¸ìŠ¤íŠ¸ê°€ ì•„ë‹ˆê±°ë‚˜ ì›ê²© ì†Œì¼“ì´ ì—†ìŠµë‹ˆë‹¤:", {
        role: connectionState.role,
        remoteSocketId: connectionState.remoteSocketId,
      });
      return;
    }

    try {
      console.log("WebRTC Offer ìƒì„± ì‹œì‘...");
      await webRTC.createOffer();
      console.log("WebRTC ì—°ê²° ì‹œì‘ë¨");
    } catch (error) {
      console.error("WebRTC ì—°ê²° ì‹œì‘ ì‹¤íŒ¨:", error);
      setRelayState((prev) => ({
        ...prev,
        error: "WebRTC ì—°ê²° ì‹œì‘ ì‹¤íŒ¨",
      }));
    }
  }, [connectionState.role, connectionState.remoteSocketId, webRTC]);

  // ìŒì„± ë¦´ë ˆì´ ì‹œì‘
  const startVoiceRelay = useCallback(() => {
    if (connectionState.role !== "host" || !webRTC.remoteStream) {
      console.warn("í˜¸ìŠ¤íŠ¸ê°€ ì•„ë‹ˆê±°ë‚˜ ì›ê²© ìŠ¤íŠ¸ë¦¼ì´ ì—†ìŠµë‹ˆë‹¤:", {
        role: connectionState.role,
        remoteStream: !!webRTC.remoteStream,
      });
      return;
    }

    const processor = audioProcessorRef.current;
    if (!processor) {
      console.error("ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤");
      return;
    }

    try {
      // ê²ŒìŠ¤íŠ¸ë¡œë¶€í„° ë°›ì€ ìŒì„±ì„ OpenAIë¡œ ì „ì†¡í•˜ë„ë¡ ì„¤ì •
      console.log("ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ ì„¤ì • ì‹œì‘...");
      console.log("ğŸ” ì›ê²© ìŠ¤íŠ¸ë¦¼ ìƒíƒœ ê²€ì¦:", {
        remoteStream: !!webRTC.remoteStream,
        remoteStreamId: webRTC.remoteStream?.id,
        audioTracks: webRTC.remoteStream?.getAudioTracks().length || 0,
        audioTrackIds: webRTC.remoteStream
          ?.getAudioTracks()
          .map((t) => ({ id: t.id, enabled: t.enabled, muted: t.muted })),
        videoTracks: webRTC.remoteStream?.getVideoTracks().length || 0,
        allTracks: webRTC.remoteStream?.getTracks().length || 0,
      });

      processor.setupInputProcessor(webRTC.remoteStream, (audioBuffer) => {
        // ì˜¤ë””ì˜¤ ë°ì´í„° ë¶„ì„
        const dataView = new DataView(audioBuffer);
        const bufferSize = audioBuffer.byteLength;
        let hasAudio = false;
        let maxAmplitude = 0;

        // PCM16 ë°ì´í„°ì—ì„œ ì§„í­ ì²´í¬ (2ë°”ì´íŠ¸ì”©)
        for (let i = 0; i < bufferSize; i += 2) {
          const sample = Math.abs(dataView.getInt16(i, true)); // little-endian
          if (sample > 100) {
            // ì„ê³„ê°’ ì„¤ì • (ë¬´ìŒ ê°ì§€)
            hasAudio = true;
          }
          maxAmplitude = Math.max(maxAmplitude, sample);
        }
        // í˜¸ìŠ¤íŠ¸ì—ì„œ ë°›ëŠ” ê²ŒìŠ¤íŠ¸ ìŒì„± ë ˆë²¨ ì—…ë°ì´íŠ¸
        const guestLevel = Math.min(100, (maxAmplitude / 32767) * 100);

        // ê²ŒìŠ¤íŠ¸ ì˜¤ë””ì˜¤ ë ˆë²¨ ì—…ë°ì´íŠ¸ (í•­ìƒ)
        setRelayState((prev) => ({
          ...prev,
          guestAudioLevel: guestLevel,
        }));

        // ì‹¤ì œ ìŒì„±ì´ ê°ì§€ë  ë•Œë§Œ ì²˜ë¦¬
        if (hasAudio) {
          console.log("âœ… ì‹¤ì œ ìŒì„± ê°ì§€ë¨, OpenAIë¡œ ì „ì†¡");
          setRelayState((prev) => ({ ...prev, currentSpeaker: "guest" }));
          openAI.sendAudioData(audioBuffer);

          // ì˜¤ë””ì˜¤ ë²„í¼ ì¹´ìš´íŠ¸ ì¦ê°€
          audioBufferCountRef.current += 1;

          // ê¸°ì¡´ íƒ€ì´ë¨¸ê°€ ìˆë‹¤ë©´ ì·¨ì†Œ
          if (conversationTimeoutRef.current) {
            clearTimeout(conversationTimeoutRef.current);
          }
          // ì¶©ë¶„í•œ ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ìŒ“ì´ê³  ì‘ë‹µ ì²˜ë¦¬ì¤‘ì´ ì•„ë‹ ë•Œë§Œ ìš”ì²­
          if (audioBufferCountRef.current * BUFFER_SIZE * 2 >= MIN_BYTES && !isProcessingResponseRef.current) {
            conversationTimeoutRef.current = setTimeout(() => {
              console.log("ğŸ¤ ëŒ€í™” ì‹œì‘ ìš”ì²­ (ë²„í¼:", audioBufferCountRef.current, ")");
              openAI.startConversation();
              isProcessingResponseRef.current = true;
              audioBufferCountRef.current = 0; // ì¹´ìš´íŠ¸ ë¦¬ì…‹
            }, 2000); // 2ì´ˆ í›„ ì‘ë‹µ ìš”ì²­
          }
        }
      });

      console.log("ìŒì„± ë¦´ë ˆì´ ì‹œì‘ë¨");
      setRelayState((prev) => ({ ...prev, isRelayActive: true }));
    } catch (error) {
      console.error("ìŒì„± ë¦´ë ˆì´ ì‹œì‘ ì‹¤íŒ¨:", error);
      setRelayState((prev) => ({
        ...prev,
        error: "ìŒì„± ë¦´ë ˆì´ ì‹œì‘ ì‹¤íŒ¨",
      }));
    }
  }, [connectionState.role, webRTC.remoteStream, openAI]);

  const clearError = useCallback(() => {
    setRelayState((prev) => ({ ...prev, error: null }));
  }, []);

  const cleanup = useCallback(() => {
    if (audioProcessorRef.current) {
      audioProcessorRef.current.cleanup();
    }
    openAI.disconnect();
  }, [openAI]);

  useEffect(() => {
    return () => {
      if (audioProcessorRef.current) {
        audioProcessorRef.current.cleanup();
      }
      openAI.disconnect();
    };
  }, []);

  return {
    ...relayState,
    webRTCState: {
      localStream: webRTC.localStream,
      remoteStream: webRTC.remoteStream,
      connectionState: webRTC.connectionState,
    },
    openAIState: {
      isConnected: openAI.isConnected,
      conversationId: openAI.conversationId,
    },

    // ì•¡ì…˜
    initialize,
    startWebRTCConnection,
    startVoiceRelay,
    setupAIAudioOutput,
    clearError,
    cleanup,
  };
}
