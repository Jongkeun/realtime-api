import { useCallback, useEffect, useRef, useState } from "react";
import { useSocket } from "./useSocket";
import { useWebRTC } from "./useWebRTC";
import { AudioProcessor } from "@/utils/audioProcessor";

interface VoiceRelayState {
  currentSpeaker: "guest" | "ai" | "none";
  error: string | null;
  microphoneLevel: number; // 0-100 ë²”ìœ„ì˜ ë§ˆì´í¬ ì…ë ¥ ë ˆë²¨ (ê²ŒìŠ¤íŠ¸ìš©)
}

export interface VoiceRelayClient extends VoiceRelayState {
  webRTCState: {
    localStream: MediaStream | null;
    remoteStream: MediaStream | null;
    connectionState: RTCPeerConnectionState;
  };
  joinAsGuest: (roomId: string) => Promise<boolean>;
  startMicrophoneMonitoring: () => void;
  clearError: () => void;
  cleanup: () => void;
}

export function useVoiceRelayGuest(): VoiceRelayClient {
  const audioProcessorRef = useRef<AudioProcessor | null>(null);

  const [relayState, setRelayState] = useState<VoiceRelayState>({
    currentSpeaker: "none",
    error: null,
    microphoneLevel: 0,
  });

  // ê° ëª¨ë“ˆ í›… ì‚¬ìš©
  const { socket, connectionState, joinRoom } = useSocket();

  const webRTC = useWebRTC(socket, connectionState.role, connectionState.remoteSocketId);

  // ê²ŒìŠ¤íŠ¸ìš© ë§ˆì´í¬ ë ˆë²¨ ëª¨ë‹ˆí„°ë§ ì‹œì‘
  const startMicrophoneMonitoring = useCallback(() => {
    if (!webRTC.localStream) {
      return;
    }

    const processor = audioProcessorRef.current;
    if (!processor) {
      console.error("ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤");
      return;
    }

    try {
      processor.setupInputProcessor(webRTC.localStream, (audioBuffer) => {
        // ì˜¤ë””ì˜¤ ë°ì´í„° ë¶„ì„
        const dataView = new DataView(audioBuffer);
        const bufferSize = audioBuffer.byteLength;
        let maxAmplitude = 0;

        // PCM16 ë°ì´í„°ì—ì„œ ì§„í­ ì²´í¬ (2ë°”ì´íŠ¸ì”©)
        for (let i = 0; i < bufferSize; i += 2) {
          const sample = Math.abs(dataView.getInt16(i, true)); // little-endian
          maxAmplitude = Math.max(maxAmplitude, sample);
        }

        // ì§„í­ì„ 0-100 ë ˆë²¨ë¡œ ë³€í™˜ (32767ì´ ìµœëŒ€ê°’)
        const level = Math.min(100, (maxAmplitude / 32767) * 100);
        console.log("ğŸ¤ !!! ê²ŒìŠ¤íŠ¸ ë§ˆì´í¬ ë ˆë²¨:", level);
        setRelayState((prev) => ({
          ...prev,
          microphoneLevel: level,
        }));
      });

      console.log("âœ… ê²ŒìŠ¤íŠ¸ ë§ˆì´í¬ ëª¨ë‹ˆí„°ë§ ì„¤ì • ì™„ë£Œ");
    } catch (error) {
      console.error("âŒ ê²ŒìŠ¤íŠ¸ ë§ˆì´í¬ ëª¨ë‹ˆí„°ë§ ì„¤ì • ì‹¤íŒ¨:", error);
    }
  }, [webRTC.localStream]);

  // ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
  useEffect(() => {
    const processor = new AudioProcessor();
    audioProcessorRef.current = processor;

    return () => {
      processor.cleanup();
    };
  }, []);

  // ìƒíƒœ ì—…ë°ì´íŠ¸
  useEffect(() => {
    if (webRTC.isConnected && webRTC.localStream) {
      startMicrophoneMonitoring();
    }

    if (webRTC.connectionState === "failed") {
      setRelayState((prev) => ({
        ...prev,
        error: "WebRTC ì—°ê²° ì‹¤íŒ¨",
      }));
    }
  }, [
    connectionState,
    webRTC.isConnected,
    webRTC.connectionState,
    webRTC.localStream,
    webRTC.remoteStream,
    startMicrophoneMonitoring,
  ]);

  // ê²ŒìŠ¤íŠ¸: ë°© ì°¸ì—¬
  const joinAsGuest = useCallback(
    async (roomId: string): Promise<boolean> => {
      try {
        // ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        const processor = audioProcessorRef.current;
        if (!processor) {
          throw new Error("ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤");
        }

        const audioInitialized = await processor.initializeAudioContext();
        if (!audioInitialized) {
          throw new Error("ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨");
        }

        // ì†Œì¼“ ë°© ì°¸ì—¬
        const success = await new Promise<boolean>((resolve, reject) => {
          joinRoom(roomId, (success, error) => {
            if (success) {
              console.log("ê²ŒìŠ¤íŠ¸ ì°¸ì—¬ ì™„ë£Œ, ë°© ID:", roomId);
              resolve(true);
            } else {
              reject(new Error(error || "ë°© ì°¸ì—¬ ì‹¤íŒ¨"));
            }
          });

          // íƒ€ì„ì•„ì›ƒ ì„¤ì •
          setTimeout(() => {
            reject(new Error("ë°© ì°¸ì—¬ íƒ€ì„ì•„ì›ƒ"));
          }, 5000);
        });

        return success;
      } catch (error) {
        console.error("ê²ŒìŠ¤íŠ¸ ì°¸ì—¬ ì‹¤íŒ¨:", error);
        setRelayState((prev) => ({
          ...prev,
          error: error instanceof Error ? error.message : "ê²ŒìŠ¤íŠ¸ ì°¸ì—¬ ì‹¤íŒ¨",
        }));
        return false;
      }
    },
    [joinRoom],
  );

  // ì—ëŸ¬ í´ë¦¬ì–´
  const clearError = useCallback(() => {
    setRelayState((prev) => ({ ...prev, error: null }));
  }, []);

  // ì „ì²´ ì‹œìŠ¤í…œ ì •ë¦¬
  const cleanup = useCallback(() => {
    if (audioProcessorRef.current) {
      audioProcessorRef.current.cleanup();
    }
  }, []);

  // ê²ŒìŠ¤íŠ¸ìš© AI ë§í•˜ê¸° ìƒíƒœ ê°ì§€
  useEffect(() => {
    if (!webRTC.remoteStream) {
      return;
    }

    let audioContext: AudioContext | null = null;
    let analyser: AnalyserNode | null = null;
    let source: MediaStreamAudioSourceNode | null = null;
    let animationId: number | null = null;

    const setupAIDetection = async () => {
      try {
        // ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        audioContext = new AudioContext();
        analyser = audioContext.createAnalyser();
        analyser.fftSize = 256;
        analyser.smoothingTimeConstant = 0.8;

        // ì›ê²© ìŠ¤íŠ¸ë¦¼ì„ ë¶„ì„ê¸°ì— ì—°ê²°
        source = audioContext.createMediaStreamSource(webRTC.remoteStream!);
        source.connect(analyser);

        const bufferLength = analyser.frequencyBinCount;
        const dataArray = new Uint8Array(bufferLength);

        let silentFrames = 0;
        const SILENT_THRESHOLD = 10; // AIê°€ ë§í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  íŒë‹¨í•  ì„ê³„ê°’
        const FRAMES_TO_SILENT = 100; // 100í”„ë ˆì„ ì—°ì† ì¡°ìš©í•˜ë©´ ë§í•˜ê¸° ì¤‘ë‹¨ìœ¼ë¡œ íŒë‹¨

        const checkAudioLevel = () => {
          if (!analyser) return;

          analyser.getByteFrequencyData(dataArray);

          // í‰ê·  ë³¼ë¥¨ ê³„ì‚°
          const average = dataArray.reduce((sum, value) => sum + value, 0) / bufferLength;

          if (average > SILENT_THRESHOLD) {
            // AIê°€ ë§í•˜ê³  ìˆìŒ
            silentFrames = 0;
            setRelayState((prev) => {
              if (prev.currentSpeaker !== "ai") {
                console.log("ğŸ¤ AI ë§í•˜ê¸° ì‹œì‘ ê°ì§€");
                return { ...prev, currentSpeaker: "ai" };
              }
              return prev;
            });
          } else {
            // ì¡°ìš©í•œ ìƒíƒœ
            silentFrames++;
            if (silentFrames >= FRAMES_TO_SILENT) {
              setRelayState((prev) => {
                if (prev.currentSpeaker === "ai") {
                  console.log("ğŸ”‡ AI ë§í•˜ê¸° ì¢…ë£Œ ê°ì§€");
                  return { ...prev, currentSpeaker: "none" };
                }
                return prev;
              });
            }
          }

          animationId = requestAnimationFrame(checkAudioLevel);
        };

        checkAudioLevel();
      } catch (error) {
        console.error("AI ê°ì§€ ì„¤ì • ì‹¤íŒ¨:", error);
      }
    };

    setupAIDetection();

    // ì •ë¦¬ í•¨ìˆ˜
    return () => {
      if (animationId) {
        cancelAnimationFrame(animationId);
      }
      if (source) {
        source.disconnect();
      }
      if (audioContext && audioContext.state !== "closed") {
        audioContext.close();
      }
    };
  }, [connectionState.role, webRTC.remoteStream]);

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      if (audioProcessorRef.current) {
        audioProcessorRef.current.cleanup();
      }
    };
  }, []);

  return {
    // ìƒíƒœ
    ...relayState,
    webRTCState: {
      localStream: webRTC.localStream,
      remoteStream: webRTC.remoteStream,
      connectionState: webRTC.connectionState,
    },

    // ì•¡ì…˜
    joinAsGuest,
    startMicrophoneMonitoring,
    clearError,
    cleanup,
  };
}
